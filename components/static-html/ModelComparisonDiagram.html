
<!DOCTYPE html>
<html>
<head><meta charset="utf-8"><title>ModelComparisonDiagram</title></head>
<body>
<div id="root"><div style="font-family:Arial, sans-serif;padding:20px;max-width:100%"><div style="margin-bottom:20px"><h2>Model Comparison for Iceberg Order Prediction</h2><p>Comparison of machine learning models evaluated on the iceberg order simulation dataset</p></div><table style="width:100%;border-collapse:collapse;margin-top:20px;font-size:14px"><thead><tr><th style="background-color:#4a90e2;color:white;padding:10px;text-align:left;border:1px solid #ddd">Model</th><th style="background-color:#4a90e2;color:white;padding:10px;text-align:left;border:1px solid #ddd">Description</th><th style="background-color:#4a90e2;color:white;padding:10px;text-align:left;border:1px solid #ddd">Parameters</th><th style="background-color:#4a90e2;color:white;padding:10px;text-align:left;border:1px solid #ddd">Strengths</th><th style="background-color:#4a90e2;color:white;padding:10px;text-align:left;border:1px solid #ddd">Limitations</th></tr></thead><tbody><tr><td style="padding:10px;border:1px solid #ddd;vertical-align:top"><strong>RandomForest</strong></td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">Ensemble of decision trees using bootstrap samples</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">n_estimators=500<br/>criterion=&#x27;log_loss&#x27;<br/>max_depth=4<br/>min_samples_split=7<br/>min_samples_leaf=3<br/>train_size=2</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">- Handles non-linearity well<br/>- Robust to outliers<br/>- Native feature importance</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">- May overfit on noisy data<br/>- Less interpretable than single trees<br/>- Memory intensive for large datasets</td></tr><tr style="background-color:#e8f4f8"><td style="padding:10px;border:1px solid #ddd;vertical-align:top"><strong>XGBoost</strong></td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">Gradient boosted trees with regularization</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">eval_metric=&#x27;error@0.5&#x27;<br/>learning_rate=0.03<br/>n_estimators=250<br/>max_depth=4<br/>gamma=0.2<br/>subsample=1.0<br/>colsample_bytree=0.8<br/>reg_alpha=0.2<br/>reg_lambda=2<br/>train_size=2</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">- High prediction accuracy<br/>- Handles imbalanced data well<br/>- Efficient implementation</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">- More prone to overfitting than RF<br/>- Requires more hyperparameter tuning<br/>- Less interpretable</td></tr><tr><td style="padding:10px;border:1px solid #ddd;vertical-align:top"><strong>LightGBM</strong></td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">Gradient boosting framework that uses tree-based algorithms</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">objective=&#x27;regression&#x27;<br/>learning_rate=0.05<br/>n_estimators=100<br/>max_depth=4<br/>num_leaves=31<br/>min_sum_hessian_in_leaf=10<br/>extra_trees=True<br/>min_data_in_leaf=100<br/>feature_fraction=1.0<br/>bagging_fraction=0.8<br/>lambda_l1=2<br/>lambda_l2=0<br/>min_gain_to_split=0.1<br/>train_size=2</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">- Faster training speed<br/>- Lower memory usage<br/>- Better accuracy with categorical features</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">- Can overfit on small datasets<br/>- Less common in production environments<br/>- Newer with less community support</td></tr><tr><td style="padding:10px;border:1px solid #ddd;vertical-align:top"><strong>LogisticRegression</strong></td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">Linear model for binary classification</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">penalty=&#x27;elasticnet&#x27;<br/>C=0.01<br/>solver=&#x27;saga&#x27;<br/>max_iter=1000<br/>l1_ratio=0.5<br/>train_size=2</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">- Simple and interpretable<br/>- Fast training and inference<br/>- Less prone to overfitting</td><td style="padding:10px;border:1px solid #ddd;vertical-align:top">- Cannot capture non-linear relationships<br/>- Lower performance ceiling<br/>- Feature engineering more important</td></tr></tbody></table><div style="background-color:#f9f9f9;border:1px solid #ddd;border-radius:5px;padding:15px;margin-top:20px"><div style="font-weight:bold;margin-bottom:10px">Evaluation Metrics Used:</div><div style="display:flex;justify-content:space-between;margin:5px 0"><span><strong>Accuracy:</strong></span><span>Overall correctness of predictions</span></div><div style="display:flex;justify-content:space-between;margin:5px 0"><span><strong>Precision:</strong></span><span>Proportion of positive identifications that were correct</span></div><div style="display:flex;justify-content:space-between;margin:5px 0"><span><strong>Recall:</strong></span><span>Proportion of actual positives that were identified correctly</span></div><div style="display:flex;justify-content:space-between;margin:5px 0"><span><strong>F1 Score:</strong></span><span>Harmonic mean of precision and recall</span></div><div style="display:flex;justify-content:space-between;margin:5px 0"><span><strong>F-beta Score:</strong></span><span>Weighted F-score with emphasis on precision (β=0.5) or recall (β=2.0)</span></div><div style="display:flex;justify-content:space-between;margin:5px 0"><span><strong>Custom Metric:</strong></span><span>max_precision_optimal_recall_score - Maximizes precision with minimum recall threshold</span></div></div></div></div>
</body>
</html>