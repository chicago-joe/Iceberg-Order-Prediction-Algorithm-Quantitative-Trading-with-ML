


from cysimdjson import JSONParser
from glob import glob
import pandas as pd
from optuna.study import get_all_study_summaries, StudySummary
from optuna import load_study
from pathlib2 import Path
import json, shutil
from collections import defaultdict


p=JSONParser()


with open("./bulk_downloads/sbuser/SX3M/SXM-449/simple_metadata.json", "rb") as f:
    jsonparsed=p.parse(f.read())


hyoptdata=jsonparsed.export()


estimator_params = defaultdict(dict)

for flat_key, value in hyoptdata.items():
    parts = flat_key.split('/')
    
    if (
        len(parts) >= 5 and
        parts[0] == "model" and
        parts[2] == "estimator" and
        parts[3] == "params"
    ):
        model_name = parts[1]
        param_name = parts[4]
        estimator_params[model_name][param_name] = value




for k in estimator_params.keys():
    model_params = estimator_params[k]
    output_path = f"{k}_estimator_params.json"
    
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w") as f:
        json.dump(model_params, f, indent=2)



model_params


plot_visualizations = glob('./bulk_downloads/sbuser/SX3M/SXM-449/**/*.html', recursive=True)


plot_visualizations


# now save images under data
target_base = "./data"

for path_str in plot_visualizations:
    src = Path(path_str)  # ✅ THIS FIXES IT
    parts = src.parts
    try:
        model_idx = parts.index("model") + 1
        model_name = parts[model_idx]
        dest_dir = Path("/".join([target_base, model_name, "visualizations"]))
        dest_dir.mkdir(parents=True, exist_ok=True)
        shutil.copy2(src, dest_dir / src.name)
        print(f"✅ Copied {src.name} to {dest_dir}")
    except Exception as e:
        print(f"❌ Failed to copy {src}: {e}")


pkl_files = glob('./bulk_downloads/sbuser/SX3M/SXM-449/**/*.pkl', recursive=True)


pkl_files





import joblib
import optuna
from optuna.storages import RDBStorage


# 2. Create an RDB backend (here: SQLite)
storage = RDBStorage(url="sqlite:///study.db")

for s in pkl_files:
    study_name = s.partition('/model/')[2].split('/')[0]+"-449"
    print(f"model study name: {study_name}")
    legacy_study = joblib.load(s)
    legacy_study.study_name = study_name
    # 3. Initialize a new study record in the DB
    study_id = storage.create_new_study(
        study_name=legacy_study.study_name,  # keep the same name, if you like
        directions=legacy_study.directions
    )
    # 4. Re‑insert existing trials from the pickle
    for trial in legacy_study.get_trials(deepcopy=False):
        storage.create_new_trial(study_id=study_id, template_trial=trial)

    # 5. (Optional) Load it with Optuna API to verify
    migrated = optuna.load_study(
        study_name=legacy_study.study_name,
        storage=storage
    )
    print(f"Number of trials migrated: {len(migrated.trials)}")


ss=get_all_study_summaries(storage,)
from optuna import Study


for study in storage.get_all_studies():
    study_name = study.study_name
    loaded_study = Study(study_name,storage)
    dfStudy = loaded_study.trials_dataframe()
    dfStudy.to_csv(f"./data/{study_name.split('-')[0]}/trials.csv",index=False)


for summary in ss:
    summary_dict = summary.best_trial.__dict__
    with open(f"./data/{summary.study_name.split('-')[0]}/best_trial.json", "w") as f:
        json.dump(summary_dict, f, indent=2, default=str)



